# Roadmap: V0 (Bounded Critique-Led Solver) → Target Governed Multi-Agent System

**Version:** 0.3  
**Date:** 2026-01-12  
**Context:** Target system must handle large, multi-stakeholder, high-uncertainty **system design and operations** problems (e.g., designing an AI-assisted operations function that continuously improves with governance).  
**Important:** The roadmap prioritizes **coverage, explicit uncertainty, auditability, and measurable learning** before adding heavy orchestration complexity. Latency/cost are treated as constraints, not the core objective.

---

## Design principles

1. **Coverage before sophistication:** early wins come from ensuring key viewpoints (risk, compliance, incentives, evaluation) are represented.
2. **Observe → measure → enforce:** do not add enforcement gates until signals are validated and baselines exist.
3. **Additive evolution:** V0 artifacts become V1+ assets (RunRecord → Ledger, Config → Registry, Known issues → Risk model).
4. **Bounded iteration by default:** loop caps prevent masking defects behind retries and keep runs debuggable.
5. **Reproducibility is a feature:** pin versions early; expand toward full manifests later.

---

## Summary timeline

- **V0**: **Bounded Critique-Led Solver** (Generate → Multi-View Critique → ≤1 Revise → Assemble → Log)
- **V1**: Structured evaluation signals & coverage metrics (still no enforcement)
- **V2**: Baselines + regression awareness (non-blocking)
- **V3**: Deterministic constraint checks + verification coverage
- **V4**: Small agent pool (specialist reviewers + synthesizer), targeted loops
- **V5**: Objective scoring + enforced regression guard + acceptance gate
- **V6**: Tool gateway + policy/safety gate + provenance
- **V7**: Full target (task graphs, blackboard branching, calibrated scoring, governed promotion)

---

## V0 — Bounded Critique-Led Solver (baseline)

**Goal:** Maximize **learning signal** per run via multi-view critique, explicit uncertainty, and traceability—under bounded iteration.

### Pipeline
Normalize → Plan (linear) → Generate → **Critique (multi-view in one call)** → (≤1 Revise) → Assemble → Log

### MVP “success” on hard problems
- **Viewpoint coverage present** (even if shallow)
- **Assumptions and unknowns labeled**
- **Actionable next steps** (what to validate/verify next)
- **RunRecords** are used to debug and improve prompts/system weekly

### Exit criteria for moving to V1
- Critique reliably flags missing roles/sections
- RunRecords are consistently persisted and consulted
- Costs/latency are within acceptable constraints (explicitly chosen)

---

## V1 — Structured evaluation signals & coverage metrics (no enforcement)

**Goal:** Turn critique and output quality into **queryable signals**.

### Additions
1. **Critique taxonomy**
   - Each issue includes: `role`, `category`, `severity`, `description`, `suggested_fix`
   - Categories: correctness, completeness, constraint_violation, clarity, uncertainty, safety
2. **Coverage score**
   - `% roles covered` + `missing roles list`
3. **Confidence heuristic**
   - low/medium/high from: blockers? major count? unverified constraints?
4. **Evaluation summary artifact**
   - Stored alongside RunRecord; **never blocks output**

### Exit criteria
- You can answer: “What failures happen most? In which roles?”
- Confidence correlates roughly with human review outcomes

---

## V2 — Baselines & regression awareness (non-blocking)

**Goal:** Build regression infrastructure **without blocking iteration**.

### Additions
1. **Prior Solutions Store (curated)**
2. **Delta computation**
   - Compare run to baseline on:
     - role coverage
     - issue severity counts
     - constraint checks (if available)
     - completeness checklist
3. **Regression annotations (non-blocking)**

### Exit criteria
- Regression signals align with reviewer judgment
- Baselines are curated (not accidental)

---

## V3 — Constraint checker + verification coverage

**Goal:** Make evaluation more deterministic.

### Additions
1. **Constraint checker**
   - deterministic checks where possible (format, required sections)
   - LLM-assisted checks with explicit “unverified” labeling
2. **Verification coverage report**
3. **Structured known issues** (impact, mitigation, verification_step)

### Exit criteria
- Contract/format failures drop sharply
- Coverage report is used in review

---

## V4 — Small agent pool (role specialization), still debuggable

**Goal:** Improve coverage and depth via separate role passes, without uncontrolled orchestration.

### Additions
- Generator
- Specialist reviewers (risk/compliance, evaluation/regression, security/ops)
- Synthesizer (merges fixes)
- Targeted iteration caps (≤2–3 total across roles)
- Minimal shared state (“mini blackboard”): candidate + issue list + resolved flags

### Exit criteria
- Specialist reviews reduce role-specific failures
- Cost increase is controlled and attributable

---

## V5 — Objective scoring + enforced regression guard + acceptance gate

**Goal:** Prevent silent quality decay and define “better” as a contract.

### Additions
- Objective model + pinned rubrics
- Mixed scoring (deterministic + rubric)
- Regression guard that can block publishing (override with logged justification)
- Acceptance thresholds for hard constraints + objectives

---

## V6 — Tool gateway + policy/safety + provenance

**Goal:** Expand capability safely (research, code, data checks).

### Additions
- Tool gateway with caching/provenance
- Policy allow/deny lists
- Snapshot non-determinism (store tool outputs)

---

## V7 — Full target governed system

**Goal:** Scalable, governed self-improvement.

### Capabilities
- Task graphs + feasibility/clarification engine
- Blackboard with branching candidates + parallel evaluation
- Calibrated objective scoring and evaluation ledger
- Solution packages (confidence, tradeoffs, compressed trace, delta vs prior, error logs)
- Governed promotion of accepted solutions to baselines

---

## Minimal coverage checklist (generic complex operations/system design)

Ensure solutions address (at minimum):

1. **Operating model**
   - roles/teams, responsibilities, workflows, handoffs
2. **Data & knowledge**
   - sources, lineage, access control, retention, quality checks
3. **Decision-making**
   - decision rights, escalation paths, approvals, audit trails
4. **Risk management**
   - limits, incident scenarios, controls, kill switches
5. **Execution & change management**
   - runbooks, rollouts, rollbacks, environment promotion, comms
6. **Compliance (high-level, if applicable)**
   - recordkeeping, approvals, oversight, audit readiness
7. **Evaluation & experimentation**
   - measurement plan, success metrics, guardrails against Goodharting
8. **Self-improvement safety**
   - objective definition, reward hacking controls, review gates, revert paths
9. **Observability**
   - metrics, dashboards, alerts, incident response
10. **Governance**
  - versioning, ownership, access control, separation of duties

---

## Notes on scope discipline

- “More roles” early should be achieved via **multi-view critique** first (cheap).
- “More agents” should be introduced only when role-specific failures are clear and recurring.
- Enforcement (regression/policy) should follow measurement and baselines, not precede them.
