# Missing Features and Roadmap

**Version:** 1.0
**Date:** 2026-01-26
**Status:** Implementation Gap Analysis

---

## Current State

The core MVP pipeline is fully functional:
- Normalize → Plan → Generate → Critique → Revise → Assemble → Log
- All 6 system invariants enforced
- CLI commands operational (`brief`, `solution`, `history`, `show`)
- Append-only RunRecord persistence

**Gap**: Missing budget/timeout enforcement and richer evaluation signals.

---

## 1. MVP Gaps (Should Be Implemented)

These features are specified in the MVP brief but not yet implemented:

| Feature | Status | Priority | Description |
|---------|--------|----------|-------------|
| Call budget enforcement | Missing | High | Cap on total LLM calls per run (spec: 2-4 default, hard cap required) |
| Timeout enforcement | Missing | High | Per-call and total run timeout with logged incidents |
| Structured repair pass | Minimal | Medium | On JSON parse failure, attempt one repair pass before fallback |
| Critique perspective coverage | Missing | Medium | Validate that critique covers minimum 6 perspectives (scope, architecture, risk, security, compliance, evaluation) |

### Implementation Notes

**Call Budget Enforcement**
```python
# In RunController
budget_config = {
    "max_llm_calls": 4,      # Hard cap
    "target_calls": 2,        # Soft target
    "max_revisions": 1,       # Already enforced
}
```

**Timeout Enforcement**
```python
# In OpenRouterClient
per_call_timeout = 30.0      # seconds
total_run_timeout = 120.0    # seconds
```

---

## 2. V1 Roadmap (Next Priority)

Structured evaluation signals and coverage metrics:

| Feature | Description |
|---------|-------------|
| Confidence score | Output confidence level (low/medium/high) based on critique severity and coverage |
| Critique taxonomy | Categorized issue types (correctness, completeness, clarity, feasibility) |
| Coverage metrics | Percentage of required perspectives covered in critique |
| Tradeoff analysis | Explicit documentation of design tradeoffs made |
| Compressed reasoning trace | Summarized decision log for debugging |

### Output Enhancement

Current `ZeusResponse`:
```python
output: str
assumptions: list[str]
known_issues: list[str]
run_id: str
```

V1 `ZeusResponse`:
```python
output: str
assumptions: list[str]
known_issues: list[str]
run_id: str
confidence: Literal["low", "medium", "high"]  # NEW
coverage_score: float                          # NEW (0.0-1.0)
tradeoffs: list[str]                          # NEW
```

---

## 3. V2+ Features (Future Versions)

| Version | Features | Description |
|---------|----------|-------------|
| **V2** | Prior Solutions Store | Store successful solutions for regression comparison |
| | Regression awareness | Detect when new solution is worse than baseline |
| | Delta computation | Show changes vs prior solutions |
| **V3** | Constraint checker | Deterministic + LLM-assisted constraint validation |
| | Verification coverage | Report on which constraints were verified |
| **V4** | Agent pool | Specialist reviewer agents (security, performance, UX) |
| | Mini blackboard | Shared state between agents |
| | Targeted iteration caps | Per-agent call limits |
| **V5** | Objective scoring | Rubric-based evaluation with calibrated scores |
| | Enforced regression guard | Block releases that fail baseline comparison |
| | Acceptance gate | Explicit pass/fail on solution quality |
| **V6** | Tool gateway | Controlled access to external tools (web, code, DB) |
| | Policy/safety gate | Allow/deny decisions with audit trail |
| **V7** | Task graphs | Non-linear task decomposition with dependencies |
| | Branching blackboard | Multiple candidate solutions explored in parallel |
| | Calibrated scoring | Cross-run comparable objective scores |

---

## 4. Target Architecture (Not in MVP Scope)

These components are designed for the full governed multi-agent system:

### Components Not Yet Built

| Component | Purpose |
|-----------|---------|
| Multi-agent orchestration | Route tasks to specialized agents from configurable pool |
| Solution blackboard | Shared state for claims, artifacts, evidence graph |
| Evaluation ledger | Persistent store for constraint reports, scorecards, critiques |
| Assumption tier classification | Classify assumptions by impact (A=blocking, B=significant, C=minor) |
| Test plan builder | Generate `test_plan.yaml` with acceptance checks |
| SolutionPackage output | Full output with Pareto alternatives, governance decisions, scoring contract |

### Target SolutionPackage Output

```python
@dataclass
class SolutionPackage:
    # Primary outputs
    output: str
    assumptions: list[str]
    known_issues: list[str]
    run_id: str

    # Evaluation (V1+)
    confidence_score: float
    tradeoff_analysis: list[Tradeoff]
    reasoning_trace: str

    # Comparison (V2+)
    prior_solution_id: str | None
    delta_summary: str | None
    regression_status: Literal["pass", "fail", "unknown"]

    # Governance (V5+)
    constraint_report: ConstraintReport
    objective_scorecard: ObjectiveScorecard
    tool_denials: list[ToolDenial]
    regression_exceptions: list[RegressionException]

    # Alternatives (V7)
    pareto_alternatives: list[AlternativeSolution]
    test_plan: TestPlan
    coverage_summary: CoverageSummary
```

---

## 5. Implementation Priority

### Phase 1: MVP Completion
1. Add call budget enforcement in `RunController`
2. Add timeout enforcement in `OpenRouterClient`
3. Improve JSON repair pass in `generate_json`
4. Add critique perspective validation in `Critic`

### Phase 2: V1 Evaluation Signals
1. Add confidence scoring based on critique severity
2. Implement coverage metrics
3. Add tradeoff extraction to assembler
4. Extend `ZeusResponse` schema

### Phase 3: V2 Regression Awareness
1. Build prior solutions store
2. Implement baseline comparison
3. Add delta computation
4. Surface regression warnings

---

## 6. Success Criteria

### MVP Complete When:
- [ ] Call budget enforced (configurable cap, logged when hit)
- [ ] Timeouts enforced (per-call and total, graceful degradation)
- [ ] JSON repair pass attempts recovery before fallback
- [ ] Critique validates minimum perspective coverage

### V1 Complete When:
- [ ] Every response includes confidence score
- [ ] Coverage metrics reported in RunRecord
- [ ] Tradeoffs explicitly documented
- [ ] Critique issues categorized by taxonomy

### V2 Complete When:
- [ ] Prior solutions stored and indexed
- [ ] Regression detection operational
- [ ] Delta reports available via CLI
